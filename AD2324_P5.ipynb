{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href="https://colab.research.google.com/github/FranciscoBPereira/AnaliseDados2324-MEI-ISEC/blob/main/AD2324_P5.ipynb" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nAoTBKXUeLe"
      },
      "outputs": [],
      "source": [
        "# Setup, Version check and Common imports\n",
        "\n",
        "# Python ≥3.8 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 8)\n",
        "\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rc('font', size=14)\n",
        "plt.rc('axes', labelsize=14, titlesize=14)\n",
        "plt.rc('legend', fontsize=14)\n",
        "plt.rc('xtick', labelsize=10)\n",
        "plt.rc('ytick', labelsize=10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "_URL = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
        "\n",
        "path_to_zip = tf.keras.utils.get_file('flower_photos', origin=_URL, untar=True, cache_dir=os.curdir)\n",
        "PATH = os.path.join(os.path.dirname(path_to_zip), 'flower_photos')"
      ],
      "metadata": {
        "id": "5M08f0BPWXqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the total number of images\n",
        "\n",
        "import pathlib\n",
        "\n",
        "data_dir = pathlib.Path(PATH)\n",
        "\n",
        "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
        "print(image_count)"
      ],
      "metadata": {
        "id": "MoskbJ2WWfMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creation of the dataset objects\n",
        "# The images in the folders are not divided in train and validation datasets\n",
        "# The following code divides samples into 80% training and 20% validation. No test set is created\n",
        "# Check details from previous class and also here: https://www.tensorflow.org/tutorials/load_data/images\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "IMG_SIZE = (180, 180)\n",
        "\n",
        "train_ds = keras.preprocessing.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  validation_split=0.2,\n",
        "  subset=\"training\",\n",
        "  seed=123,\n",
        "  shuffle=True,\n",
        "  image_size=IMG_SIZE,\n",
        "  batch_size=batch_size)\n",
        "\n",
        "val_ds = keras.preprocessing.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  validation_split=0.2,\n",
        "  subset=\"validation\",\n",
        "  seed=123,\n",
        "  shuffle=True,\n",
        "  image_size=IMG_SIZE,\n",
        "  batch_size=batch_size)\n",
        "\n",
        "class_names = train_ds.class_names\n",
        "\n",
        "train_ds = train_ds.cache().prefetch(1)\n",
        "val_ds = val_ds.cache().prefetch(1)\n"
      ],
      "metadata": {
        "id": "brHkYy_SWpyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain information from the dataset\n",
        "\n",
        "# Cardinality\n",
        "print('Cardinalidade Treino: ', train_ds.cardinality().numpy())\n",
        "print('Cardinalidade Validacão: ', val_ds.cardinality().numpy())\n",
        "\n",
        "print('Nr. of classes: ', len(class_names))\n",
        "print('Classes: ', class_names)\n",
        "\n",
        "# Explain the cardinality values"
      ],
      "metadata": {
        "id": "Wi6B_53mWuzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize a few examples\n",
        "\n",
        "plt.figure(figsize=(12, 12))\n",
        "for images, labels in train_ds.take(1):\n",
        "    for i in range(9):\n",
        "        plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(images[i].numpy()/255.)\n",
        "        plt.title(class_names[labels[i]])\n",
        "        plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "mV1GEgwIW3Ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creation of a baseline CNN\n",
        "\n",
        "# It must comply with the following constraints:\n",
        "#   1. Use Keras Functional API\n",
        "#   2. Maximum of 5 million parameters\n",
        "#   3. Without Data Augmentation\n",
        "#   4. Do not forget Input Rescaling\n",
        "\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/Model\n",
        "# https://www.tensorflow.org/guide/keras/sequential_model\n",
        "# https://keras.io/api/layers/\n",
        "\n",
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# FUNCTIONAL API\n",
        "\n",
        "inputs = keras.Input(shape=(180,180,3))\n",
        "\n",
        "a = layers.Rescaling(scale = 1./255)(inputs)\n",
        "\n",
        "### Complete Model ###\n",
        "### Complete Model ###\n",
        "\n",
        "outputs = layers.Dense(5, activation=\"softmax\")(### Complete Call ###)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Sh5__FWUXDeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the limit for the maximum number of parameters\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "CvUBRk7oXTrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model compilation\n",
        "\n",
        "#  1. Select the loss function suitable for this situation\n",
        "#  2. Adopt ADAM optimizer, with default parameterization\n",
        "#  3. Select accuracy metric to evaluate the model\n",
        "\n",
        "L = ### Complete ###\n",
        "\n",
        "model.compile(loss=L, optimizer=\"adam\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "0vl-jCe5XZWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train for 20 epochs\n",
        "\n",
        "history = model.fit(\n",
        "  train_ds,\n",
        "  validation_data=val_ds,\n",
        "  epochs=20\n",
        ")"
      ],
      "metadata": {
        "id": "0nUxWwCyXpqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize results (both accuracy and loss)\n",
        "\n",
        "import pandas as pd\n",
        "history_frame = pd.DataFrame(history.history)\n",
        "history_frame.loc[:, ['loss', 'val_loss']].plot()\n",
        "history_frame.loc[:, ['accuracy', 'val_accuracy']].plot()"
      ],
      "metadata": {
        "id": "UUOMrV8pYAhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Analyze results. By looking at each one of the charts, what do you think is happening?\n"
      ],
      "metadata": {
        "id": "xrAqPAPS52rK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add Data Augmentation to fight overfitting - Model DA1\n",
        "\n",
        "# Details can be found here:\n",
        "# https://keras.io/api/layers/preprocessing_layers/image_augmentation/\n",
        "# https://www.tensorflow.org/tutorials/images/data_augmentation\n",
        "\n",
        "# The data augmentation module is created using the Sequential API\n",
        "\n",
        "data_augmentation1 = keras.Sequential([\n",
        "    keras.layers.RandomFlip(mode=\"horizontal\"),\n",
        "    keras.layers.RandomRotation(factor=0.5),\n",
        "])\n",
        "\n"
      ],
      "metadata": {
        "id": "v-S9UHq7YEDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize examples of augmented images\n",
        "\n",
        "plt.figure(figsize=(20, 20))\n",
        "for images, _ in train_ds.take(1):\n",
        "  for i in range(16):\n",
        "    augmented_images = data_augmentation1(images)\n",
        "    ax = plt.subplot(4, 4, i + 1)\n",
        "    plt.imshow(augmented_images[0].numpy()/255.)\n",
        "    plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "Mj7pWf-HZafW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model DA1 with data augmentation\n",
        "\n",
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# API FUNCIONAL\n",
        "\n",
        "inputs = keras.Input(shape=(180,180,3))\n",
        "\n",
        "\n",
        "### Complete Model ###\n",
        "\n",
        "\n",
        "model_DA1 = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "r_x-AFDIEd_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile and train Model 2\n",
        "\n",
        "L = ### Complete ###\n",
        "\n",
        "model_DA1.compile(loss=L, optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "history = model_DA1.fit(\n",
        "  train_ds,\n",
        "  validation_data=val_ds,\n",
        "  epochs=20\n",
        ")"
      ],
      "metadata": {
        "id": "1XwK8C_GEufp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize results (both accuracy and loss)\n",
        "\n",
        "import pandas as pd\n",
        "history_frame = pd.DataFrame(history.history)\n",
        "history_frame.loc[:, ['loss', 'val_loss']].plot()\n",
        "history_frame.loc[:, ['accuracy', 'val_accuracy']].plot()"
      ],
      "metadata": {
        "id": "dnNyDTkYDvbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How do you analyze results?\n",
        "\n",
        "Are they similar to those obtained by the previous model?\n"
      ],
      "metadata": {
        "id": "WkpruFAxFLBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Quiz: Create a new data augmentation module - Model DA2\n",
        "\n",
        "# It should comprise 3 pre-processing layers, where, at least, two of them, must be different from the ones already used\n",
        "\n",
        "# Add the pre-processing module to the beggining of the NN\n",
        "# Compile and train the modified CNN and analyze results\n",
        "\n",
        "\n",
        "# CODE GOES HERE - MODEL DA2\n",
        "\n"
      ],
      "metadata": {
        "id": "FTg7TQkgEGFT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
